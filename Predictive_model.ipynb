{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZeroToH3ro/Predictive-Model-Advance/blob/main/Predictive_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nDqgslP2awmd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fpPnKa0ec5JD"
      },
      "outputs": [],
      "source": [
        "def sequence_to_features(seq, seq_type='dna', daas=None):\n",
        "    \"\"\"\n",
        "    seq_type='dna' will apply the DNA mapping,\n",
        "    seq_type='aa'  will apply the amino acid mapping.\n",
        "    daas is 1 if the medicine is used, else 0.\n",
        "    \"\"\"\n",
        "    # DNA base mapping\n",
        "    base_dict = {\n",
        "        'A': 1, 'C': 2, 'G': 3, 'T': 4,\n",
        "        'N': 0, '-': 0,\n",
        "        'R': 5, 'Y': 6, 'M': 7, 'K': 8,\n",
        "        'S': 9, 'W': 10, 'H': 11, 'B': 12,\n",
        "        'V': 13, 'D': 14\n",
        "    }\n",
        "\n",
        "    # Amino acid mapping\n",
        "    aa_dict = {\n",
        "        'A': 1, 'R': 2, 'N': 3, 'D': 4, 'C': 5,\n",
        "        'Q': 6, 'E': 7, 'G': 8, 'H': 9, 'I': 10,\n",
        "        'L': 11, 'K': 12, 'M': 13, 'F': 14, 'P': 15,\n",
        "        'S': 16, 'T': 17, 'W': 18, 'Y': 19, 'V': 20,\n",
        "        '-': 0, 'X': 0\n",
        "    }\n",
        "\n",
        "    features = []\n",
        "    if seq_type == 'dna':\n",
        "        features = [base_dict.get(base, 0) for base in seq]\n",
        "    elif seq_type == 'aa':\n",
        "        features = [aa_dict.get(aa, 0) for aa in seq]\n",
        "    else:\n",
        "        raise ValueError(\"seq_type must be either 'dna' or 'aa'\")\n",
        "\n",
        "    # Include DAAS (medicine) as a binary feature if provided\n",
        "    if daas is not None:\n",
        "        features.append(int(daas))  # 1 if True, else 0\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lmliD8lm6TWy"
      },
      "outputs": [],
      "source": [
        "def prepare_data_nucleotide(data_path):\n",
        "    # Load dataset\n",
        "    data = pd.read_excel(data_path)\n",
        "\n",
        "    # Build feature matrix X using Nucleotide + DAAS\n",
        "    X_list = []\n",
        "    for idx, row in data.iterrows():\n",
        "        nucleotide_seq = str(row['Nucleotide']) if pd.notna(row['Nucleotide']) else ''\n",
        "        # Convert the DAAS column to 1 if not empty (or if the user took the medicine)\n",
        "        daas_value = row['DAAS'] if 'DAAS' in data.columns else None\n",
        "\n",
        "        # Create features\n",
        "        features = sequence_to_features(\n",
        "            seq=nucleotide_seq,\n",
        "            seq_type='dna',\n",
        "            daas=daas_value\n",
        "        )\n",
        "        X_list.append(features)\n",
        "\n",
        "    # Convert list of lists into a 2D numpy array\n",
        "    X = np.array(X_list, dtype=float)\n",
        "\n",
        "    # Target vector y from \"Respond\"\n",
        "    # Modified to map specific values, check for potential issues\n",
        "    # For example, 'Yes'/'No' mapping. Adjust as per your data\n",
        "    y = data['Respond'].map({'Yes': 1, 'No': 0}).values\n",
        "\n",
        "    # Add a check for unique values in y:\n",
        "    unique_classes = np.unique(y)\n",
        "    if len(unique_classes) < 2:\n",
        "        raise ValueError(f\"Target variable 'Respond' has only {len(unique_classes)} unique class(es): {unique_classes}. \"\n",
        "                         f\"At least two classes are needed for classification.\")\n",
        "\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MCxXur2E6fPB"
      },
      "outputs": [],
      "source": [
        "def prepare_data_amino(data_path):\n",
        "    # Load dataset\n",
        "    data = pd.read_excel(data_path)\n",
        "\n",
        "    # Build feature matrix X using Amino_Acid + DAAS\n",
        "    X_list = []\n",
        "    for idx, row in data.iterrows():\n",
        "        aa_seq = str(row['Amino_Acid']) if pd.notna(row['Amino_Acid']) else ''\n",
        "        daas_value = row['DAAS'] if 'DAAS' in data.columns else None\n",
        "\n",
        "        # Create features\n",
        "        features = sequence_to_features(\n",
        "            seq=aa_seq,\n",
        "            seq_type='aa',\n",
        "            daas=daas_value\n",
        "        )\n",
        "        X_list.append(features)\n",
        "\n",
        "    X = np.array(X_list, dtype=float)\n",
        "\n",
        "    # Target vector y from \"Respond\"\n",
        "    y = data['Respond'].map({'Yes': 1, 'No': 0}).values # Assume your data has 'Yes' and 'No' values.\n",
        "\n",
        "    # Add a check for unique values in y:\n",
        "    unique_classes = np.unique(y)\n",
        "    if len(unique_classes) < 2:\n",
        "        raise ValueError(f\"Target variable 'Respond' has only {len(unique_classes)} unique class(es): {unique_classes}. \"\n",
        "                         f\"At least two classes are needed for classification.\")\n",
        "\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Io7m-Lc7IpoP"
      },
      "outputs": [],
      "source": [
        "def plot_model_comparison(results):\n",
        "    \"\"\"\n",
        "    Create visualizations comparing model performance\n",
        "    \"\"\"\n",
        "    # Accuracy comparison\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    accuracies = {name: res['accuracy'] for name, res in results.items()}\n",
        "    plt.bar(accuracies.keys(), accuracies.values())\n",
        "    plt.title('Model Accuracy Comparison')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # ROC curves\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for name, res in results.items():\n",
        "        # Get the stored fpr, tpr values directly\n",
        "        fpr = res['roc_curve'][0]  # First element is fpr\n",
        "        tpr = res['roc_curve'][1]  # Second element is tpr\n",
        "        auc_score = res['auc']     # Get the pre-calculated AUC score\n",
        "\n",
        "        plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--')  # Add diagonal line\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves for Different Models')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RnuaUd-nuzNj"
      },
      "outputs": [],
      "source": [
        "def optimize_models(X_train_scaled, y_train):\n",
        "    \"\"\"\n",
        "    Optimize hyperparameters for all models using GridSearchCV\n",
        "    \"\"\"\n",
        "    optimized_models = {}\n",
        "\n",
        "    # SVM optimization\n",
        "    svm_params = {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'gamma': ['scale', 'auto', 0.1, 0.01, 0.001],\n",
        "        'kernel': ['rbf', 'linear', 'poly'],\n",
        "        'class_weight': ['balanced', None]\n",
        "    }\n",
        "    svm = GridSearchCV(SVC(probability=True, random_state=42), svm_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "    # Neural Network optimization\n",
        "    nn_params = {\n",
        "        'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50), (100, 100)],\n",
        "        'activation': ['relu', 'tanh'],\n",
        "        'alpha': [0.0001, 0.001, 0.01],\n",
        "        'learning_rate': ['constant', 'adaptive'],\n",
        "        'max_iter': [2000]\n",
        "    }\n",
        "    nn = GridSearchCV(MLPClassifier(random_state=42), nn_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "    # KNN optimization\n",
        "    knn_params = {\n",
        "        'n_neighbors': [3, 5, 7, 9],\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan'],\n",
        "        'algorithm': ['auto', 'ball_tree', 'kd_tree']\n",
        "    }\n",
        "    knn = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "    # Logistic Regression optimization\n",
        "    lr_params = {\n",
        "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "        'penalty': ['l1', 'l2'],\n",
        "        'solver': ['liblinear', 'saga'],\n",
        "        'class_weight': ['balanced', None]\n",
        "    }\n",
        "    lr = GridSearchCV(LogisticRegression(random_state=42, max_iter=2000), lr_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "    # Random Forest optimization\n",
        "    rf_params = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [10, 20, 30, None],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'class_weight': ['balanced', 'balanced_subsample', None]\n",
        "    }\n",
        "    rf = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "    # GBM optimization\n",
        "    gbm_params = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'learning_rate': [0.01, 0.1, 0.3],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'min_samples_split': [2, 5],\n",
        "        'subsample': [0.8, 0.9, 1.0]\n",
        "    }\n",
        "    gbm = GridSearchCV(GradientBoostingClassifier(random_state=42), gbm_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "    # FDA optimization\n",
        "    fda_params = {\n",
        "        'solver': ['svd', 'lsqr', 'eigen'],\n",
        "        'shrinkage': [None, 'auto'],\n",
        "        'store_covariance': [True, False]\n",
        "    }\n",
        "    fda = GridSearchCV(LinearDiscriminantAnalysis(), fda_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "    models = {\n",
        "        'SVM': svm,\n",
        "        'Neural Network': nn,\n",
        "        'KNN': knn,\n",
        "        'Logistic Regression': lr,\n",
        "        'Random Forest': rf,\n",
        "        'FDA': fda,\n",
        "        'GBM': gbm\n",
        "    }\n",
        "\n",
        "    # Fit all models\n",
        "    for name, model in models.items():\n",
        "        print(f\"Optimizing {name}...\")\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        optimized_models[name] = model.best_estimator_\n",
        "        print(f\"Best parameters for {name}: {model.best_params_}\")\n",
        "        print(f\"Best cross-validation score: {model.best_score_:.4f}\\n\")\n",
        "\n",
        "    return optimized_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Cp6Pcw93dJZw"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_models(X, y, label=''):\n",
        "    # Split data with stratification\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Get optimized models\n",
        "    models = optimize_models(X_train_scaled, y_train)\n",
        "\n",
        "    results = {}\n",
        "    for name, model in models.items():\n",
        "        # Train and predict\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        report = classification_report(y_test, y_pred)\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "        auc_score = auc(fpr, tpr)\n",
        "\n",
        "        # Store results\n",
        "        results[name] = {\n",
        "            'accuracy': accuracy,\n",
        "            'report': report,\n",
        "            'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
        "            'roc_curve': (fpr, tpr),\n",
        "            'auc': auc_score\n",
        "        }\n",
        "\n",
        "\n",
        "        print(f\"\\nResults for {name}:\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"AUC Score: {auc_score:.4f}\")\n",
        "        print(\"Classification Report:\")\n",
        "        print(report)\n",
        "\n",
        "    # Plot comparisons\n",
        "    plot_model_comparison(results)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        },
        "id": "0fqcBcjYdZD0",
        "outputId": "59dc4c10-aa83-4979-992c-878796cdb2a2"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing SVM...\n",
            "Best parameters for SVM: {'C': 1, 'class_weight': 'balanced', 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Best cross-validation score: 0.8697\n",
            "\n",
            "Optimizing Neural Network...\n",
            "Best parameters for Neural Network: {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100, 50), 'learning_rate': 'constant', 'max_iter': 2000}\n",
            "Best cross-validation score: 0.8039\n",
            "\n",
            "Optimizing KNN...\n",
            "Best parameters for KNN: {'algorithm': 'auto', 'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'uniform'}\n",
            "Best cross-validation score: 0.8420\n",
            "\n",
            "Optimizing Logistic Regression...\n",
            "Best parameters for Logistic Regression: {'C': 0.001, 'class_weight': None, 'penalty': 'l1', 'solver': 'saga'}\n",
            "Best cross-validation score: 0.8416\n",
            "\n",
            "Optimizing Random Forest...\n",
            "Best parameters for Random Forest: {'class_weight': 'balanced', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 200}\n",
            "Best cross-validation score: 0.8602\n",
            "\n",
            "Optimizing FDA...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-63a329c1fe91>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 1) Prepare Nucleotide-based dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_nu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_nu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data_nucleotide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mresults_nu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_nu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_nu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Nucleotide'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# 2) Prepare Amino_Acid-based dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-327456fbd404>\u001b[0m in \u001b[0;36mtrain_and_evaluate_models\u001b[0;34m(X, y, label)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Get optimized models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-44f59de2abf2>\u001b[0m in \u001b[0;36moptimize_models\u001b[0;34m(X_train_scaled, y_train)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Optimizing {name}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0moptimized_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best parameters for {name}: {model.best_params_}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1021\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1570\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    967\u001b[0m                     )\n\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    970\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    971\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Path to your actual Excel data\n",
        "data_path = \"sample_data/134-samples-aligned-cutted-Nu-and-a.a-17.12.24.xlsx\"\n",
        "\n",
        "# 1) Prepare Nucleotide-based dataset\n",
        "X_nu, y_nu = prepare_data_nucleotide(data_path)\n",
        "results_nu = train_and_evaluate_models(X_nu, y_nu, label='Nucleotide')\n",
        "\n",
        "# 2) Prepare Amino_Acid-based dataset\n",
        "X_aa, y_aa = prepare_data_amino(data_path)\n",
        "results_aa = train_and_evaluate_models(X_aa, y_aa, label='Amino_Acid')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1ln2U64ruT1jzf7nIerbs1_saVD6H83Fe",
      "authorship_tag": "ABX9TyOEVfUGZc09Rn7v0W29zO/y",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}